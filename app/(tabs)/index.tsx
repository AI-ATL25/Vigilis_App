import { ElevenLabsClient } from "@elevenlabs/elevenlabs-js";
import { Audio } from 'expo-av';
import React, { useEffect, useState } from 'react';
import { Button, Platform, ScrollView, StyleSheet, Text, View } from 'react-native';

// Define types for ElevenLabs API responses
type SingleChannelResponse = {
    content: string;
};

type MultiChannelResponse = {
    channels: Array<{ content: string }>;
};

type TranscriptionResponse = SingleChannelResponse | MultiChannelResponse;

// Note: Store this in your environment variables or app config
const ELEVENLABS_API_KEY = process.env.EXPO_PUBLIC_ELEVENLABS_API_KEY;
console.log(ELEVENLABS_API_KEY)

// Initialize the ElevenLabs client once
const elevenLabs = new ElevenLabsClient({
  apiKey: ELEVENLABS_API_KEY,
});

export default function App() {
  const [recording, setRecording] = useState<Audio.Recording | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState("Tap 'Start Recording' to begin...");
  const [isLoading, setIsLoading] = useState(false);
  const [permissionResponse, requestPermission] = Audio.usePermissions();

  useEffect(() => {
    async function setupAudio() {
      try {
        if (Platform.OS !== 'web') {
          // For mobile, we need to ensure audio mode is set up first
          await Audio.setAudioModeAsync({
            allowsRecordingIOS: true,
            playsInSilentModeIOS: true,
            staysActiveInBackground: true,
          });
        }
        
        // Then request permissions
        if (!permissionResponse?.granted) {
          const { granted } = await requestPermission();
          if (!granted) {
            console.warn('Permission not granted for audio recording');
          }
        }
      } catch (err) {
        console.error('Error setting up audio:', err);
      }
    }

    setupAudio();
  }, []);

  async function startRecording() {
    try {
      // Double-check permissions
      if (!permissionResponse?.granted) {
        const { granted } = await requestPermission();
        if (!granted) {
          throw new Error('Microphone permission is required!');
        }
      }

      // For mobile platforms, ensure audio mode is set
      if (Platform.OS !== 'web') {
        await Audio.setAudioModeAsync({
          allowsRecordingIOS: true,
          playsInSilentModeIOS: true,
          staysActiveInBackground: true,
        });
      }

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );
      setRecording(recording);
      setIsRecording(true);
      setTranscript('Recording...');
    } catch (err) {
      const error = err as Error;
      console.error('Failed to start recording', error);
      
      let errorMessage = 'Error starting recording: ';
      if (Platform.OS !== 'web' && error.message.includes('No media devices')) {
        errorMessage += 'Microphone not found or not accessible. Please ensure your device has a working microphone and the app has permission to use it.';
      } else {
        errorMessage += error.message;
      }
      
      setTranscript(errorMessage);
      setIsRecording(false);
    }
  }

  async function stopRecording() {
    if (!recording) return;
    
    setIsRecording(false);
    setIsLoading(true);
    setTranscript('Stopping recording and preparing for transcription...');

    try {
      await recording.stopAndUnloadAsync();
      const uri = recording.getURI();
      setRecording(null);

      if (!uri) {
        throw new Error("Could not retrieve recorded audio URI.");
      }

      setTranscript('Uploading audio to ElevenLabs...');
      const transcribedText = await uploadAndTranscribe(uri);
      setTranscript(transcribedText);

    } catch (err) {
      const error = err as Error;
      console.error('Failed to stop recording or transcribe:', error);
      setTranscript(`Transcription failed: ${error.message}`);
    } finally {
      setIsLoading(false);
    }
  }


/**
 * Uploads the recorded audio file to ElevenLabs for transcription.
 * @param {string} fileUri The local file URI (e.g., 'file:///data/user/...') generated by expo-av.
 * @returns {Promise<string>} The transcribed text.
 */
async function uploadAndTranscribe(fileUri: string): Promise<string> {
    try {
        // Read the file and ensure correct MIME type
        const fetchResponse = await fetch(fileUri);
        let audioBlob = await fetchResponse.blob();
        
        // Create a new blob with explicit audio MIME type
        audioBlob = new Blob([audioBlob], { 
            type: 'audio/mp4' // Match the recording format
        });
        
        console.log('File URI:', fileUri);
        console.log('Content type:', audioBlob.type);

        console.log('Audio blob size:', audioBlob.size, 'bytes');

        // Call the ElevenLabs API with the correct model ID
        const result = await elevenLabs.speechToText.convert({
            file: audioBlob,
            modelId: "scribe_v1", // ElevenLabs' model ID
            languageCode: "en", // Using standard language code
            diarize: false, // Simplifying the request
        });

        console.log('API Response:', JSON.stringify(result, null, 2));

        // Try to extract text from various response formats
        if (result && typeof result === 'object') {
            // Check for direct text property
            if ('text' in result && typeof result.text === 'string') {
                return result.text;
            }
            
            // Check for content property
            if ('content' in result && typeof result.content === 'string') {
                return result.content;
            }

            // Check for transcript property
            if ('transcript' in result && typeof result.transcript === 'string') {
                return result.transcript;
            }

            // Check for channels array
            if ('channels' in result && Array.isArray(result.channels)) {
                const texts = result.channels
                    .map(channel => channel.content || channel.text || channel.transcript)
                    .filter(Boolean);
                if (texts.length > 0) {
                    return texts.join(' ');
                }
            }

            // If we got a response but couldn't find text, log it for debugging
            console.log('Unexpected response structure:', result);
            return `Could not extract text from response: ${JSON.stringify(result)}`;
        }

        return "No transcription content received. Check console for details.";

    } catch (err) {
        const error = err as Error;
        console.error("ElevenLabs Transcription Error:", error);
        throw new Error(`Failed to transcribe audio: ${error.message}`);
    }
}

  // NOTE: The actual API call function (uploadAndTranscribe) requires handling 
  // file-to-binary conversion and a multipart form data POST request, 
  // which is complex for a simple front-page example.
  // It would use 'fetch' with FormData to send the audio file and API key 
  // to the ELEVENLABS_API_URL.

  return (
    <View style={styles.container}>
      <Text style={styles.title}>ðŸŽ¤ Caller</Text>
      <View style={styles.buttonContainer}>
        <Button
          title={isRecording ? 'Recording...' : 'Start Recording'}
          onPress={startRecording}
          disabled={isRecording || isLoading}
          color={isRecording ? 'red' : 'green'}
        />
        <Button
          title="Stop & Transcribe"
          onPress={stopRecording}
          disabled={!isRecording || isLoading}
          color="darkorange"
        />
      </View>
      
      <View style={styles.transcriptBox}>
        <Text style={styles.statusText}>{isLoading ? 'Processing...' : 'Transcription:'}</Text>
        <ScrollView style={styles.transcriptScroll}>
          <Text style={styles.transcriptText}>{transcript}</Text>
        </ScrollView>
      </View>

      <Text style={styles.footerText}>
        {Platform.OS === 'ios' ? 'Microphone permission status: ' : 'Permission status: '}
        {permissionResponse?.status}
      </Text>
    </View>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    justifyContent: 'center',
    padding: 20,
    backgroundColor: '#f5f5f5',
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    marginBottom: 30,
    textAlign: 'center',
    color: '#333',
  },
  buttonContainer: {
    flexDirection: 'row',
    justifyContent: 'space-around',
    marginBottom: 30,
  },
  transcriptBox: {
    backgroundColor: '#fff',
    borderRadius: 10,
    padding: 15,
    minHeight: 150,
    elevation: 3,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    marginBottom: 20,
  },
  statusText: {
    fontSize: 16,
    fontWeight: '600',
    color: '#555',
    marginBottom: 10,
    borderBottomWidth: 1,
    borderBottomColor: '#eee',
    paddingBottom: 5,
  },
  transcriptScroll: {
    maxHeight: 100, // Limit scroll view height
  },
  transcriptText: {
    fontSize: 16,
    lineHeight: 24,
    color: '#333',
  },
  footerText: {
    textAlign: 'center',
    fontSize: 12,
    color: '#888',
    marginTop: 10,
  }
});